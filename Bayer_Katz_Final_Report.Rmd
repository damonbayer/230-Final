---
title: "Gibbs Sampling with Data Augmentation for Bayesian Analysis of Binary and Polychotomous Response Data"
subtitle: "STAT 230 Final Report"
author: "Damon Bayer & Corey Katz"
date: "12/16/2019"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    df_print: kable
header-includes:
  - \usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
  - \usepackage{amsmath}
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, cache=T)
library(tidyverse)
library(here)
library(knitr)
source(here("plotting_functions.R"))

options(readr.num_columns = 0)

post_sample_summary <- function(post) {
  post %>% 
  pivot_longer(everything()) %>%
  mutate(name = as_factor(name)) %>% 
  rename(Variable = name) %>% 
  group_by(Variable) %>% 
  summarize_all(list(mean = mean,
                     sd = sd,
                     "2.5%" = function(x) quantile(x, probs = 0.025),
                     "50%" = median,
                     "97.5%" = function(x) quantile(x, probs = 0.975)))
}
```

# Abstract

Here's the abstract.

# Introduction

Markov Chain Monte Carlo (MCMC) methods revolutionize Bayesian inference, making it possible to sample from complex posterior distributions. As computing power continues to evolve, so do sampling methods, as well as the complexity of models scientist try to fit. While advances have been made to MCMC, sampling from intractable posterior distributions is still a difficult and time-consuming task. Gibbs Sampling is one of the most common forms of MCMC and one of the earliest attempts to alleviate these struggles of sampling from intractable posteriors was the development of data-augmentation Gibbs Sampling to regression models. 

@albertchib93 was the first paper to introduce using data augmentation to fit Bayesian probit regression models to binary and  polychotomous data.  They built upon the foundation of Gibbs Sampling built by @gelfand1990sampling and @tannerwong1987. Adding latent variables has been used in many other areas of statistics and mathematics to help with computing.  By introducing a latent variable to the probit regression model of a binary response variable (and its extensions), they were able to simplify the Gibbs Sampling process and make sampling from the posterior distribution easier. This paper laid the groundwork for using data-augmentation Gibbs Sampling for Bayesian inference by enabling the computation of exact posterior distributions of regression coefficients for binary and polychotomous response.

Compared to traditional maximum likelihood estimation (MLE), Albert and Chibâ€™s approach is advantageous in the case of small data, where MLE can be biased, as well as in the case of regression models with complicated likelihoods, such as in the multivariate probit case. Additionally, this method enables the modeling of the marginal distribution of residuals, which are on a continuous scale and can be more helpful for outlier detection than frequentist residuals, which only take on two values.

This paper focuses on implementations and applications of several of the algorithms presented in @albertchib93, including the aforementioned data-augmentated Gibbs sampling for probit regression on binary response data, the multinomial probit regression on ordinal response data, and the t-link regression on binary response data. We will also compare these method to the MLE estimates equivalent methods (Generalized Linear Models), as well as a more modern MCMC method, No-U-Turn Samplers through the implementation on Stan. By comparing these three methods, we can better understand the importance of the introduction of Data-Augmentation to Bayesian inference.


# Methodology
<!-- (explaining the algorithm), -->

In this section, we detail each method for data-augmented Gibbs sampling, including pseudocode for each algorithm.

## Probit Model 

The first model that data augmentation can be used to find posterior distributions is the probit model. The following is a standard Bayesian Probit model with non informative priors on the regression coefficients: 

\begin{equation}
y_i| \pi_{i},\vec{\beta}, \vec x_i \sim Bernoulli(\pi_{i})  \ \ \ \text{for} \ \  i = 0,...,n
\end{equation}

\begin{equation}
\pi_{i} = \Phi(x_i^T\beta)
\end{equation}

\begin{equation}
\beta_j \sim N(0, 100),  \ \ \ \text{for} \  j = 0,...,p
\end{equation}


Note that $\Phi$ is the cumulative density function of the normal distribution. 

The idea of data augmentation is to introduce a continuous latent variable derived from a binary response in order to make sampling from the posterior distributions of the probit model coefficients easier. By introducing independent latent variables $Z_1, ... Z_n$, we can now find the joint posterior of $\vec \beta$ and $\vec Z$ given Y. We can then marginalize over the posterior distribution of $\vec Z|\vec Y$ and thus we have the conditional posterior of $\beta|\vec{Z},\vec{Y}$. This method is further simplified if we assume (as we did above) non-informative priors on the regression coefficients, $\vec \beta$ [@albertchib93]. 

With this approach we are able to create a Gibbs sampler that only needs to sample from truncated normal distributions and multivariate normal distribution, which are extremely easy with today's computing power. Based on the fully conditonal posterior of $\beta|\vec{Z},\vec{Y}$, Albert and Chib equated this method of probit regression on binary Y to doing linear regression on the latent variable Z. This will be evident in the algortim as we use the `lm` function to find the mean of the posterior distribution of $\beta|\vec{Z},\vec{Y}$ [@albertchib93]. 

### Algorithm: 

\begin{algorithm}[H]
\caption{Probit Regression Using Gibbs Sampler with Data Augmentation}
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\vec{Y}$,$\textbf{X}$}
\Output{Posterior Samples of Regression Coeffiecents, $\vec \beta$}
\BlankLine
Set Number of Samples (Total ($N_s$) and Burn-in) 
Initailize $\vec \beta^{(0)}$ \\
Set $\Sigma = (X^TX)^{-1}$
n = Number of Observations
\BlankLine
\For{k = 1 to $N_s$}{
  \For{i = 1 to n}{
    \eIf{$y_i$ = 1}{
        Sample $z_i^{(k)}$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,1,0,$\infty$)\;
    }{
        Sample $z_i^{(k)}$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,1,$-\infty$,0)\;
    }
  }
  Regress $\vec Z$ onto $\textbf{X}$ to find $\vec \beta_Z$ \\
  Sample $\beta^{(k)}|\vec Z$ from $\mathcal{N}(\hat{\vec \beta_Z}^{(k)}, \textbf{$\Sigma$})$
  
  
}
\end{algorithm}
  
## T-link to Approximate Logistic Regression

In this section we will discuss an extension of the Gibbs sampler discussed for the probit model. The purpose of this model is to use data augmentation with t-distributions to approximate the logistic regression model for a binary response. The simple Bayesian Logistic Regression model with non-informative priors is as follows: 

\begin{equation}
y_i| \pi_{i},\vec{\beta}, \vec x_i \sim Bernoulli(\pi_{i})  \ \ \ \text{for} \ \  i = 0,...,n
\end{equation}

\begin{equation}
\text{logit}(\pi_{i}) = x_i^T\beta
\end{equation}

\begin{equation}
\beta_j \sim N(0, 100),  \ \ \ \text{for} \  j = 0,...,p
\end{equation}

Where $\text{logit}(p) = \frac{p}{1-p}$ 

The T-link is an extension of the probit regression model because instead of using the normal cdf (as in the probit model) we use the t-distribution cdf as our link function. If we replace $\Phi$ with the cdf of the t($\nu$) in (2), we would have Bayesian t-link model. By generalizing the model, we can now choose a degrees of freedom, $\nu$ where t($\nu$) better fits out model and thus a link function that approximates other well-known link functions. For example, we are usually concerned with logistic regression because of the interpretability of the coefficients into odds ratios. The flexibility of the t-link function allows us to draw from a posterior distribution of $\vec \beta$ that is approximately the posterior distribution if we had chosen to use the logit link function. Note that if we set the degrees of freedom equal to infinity, we would revert back to the probit model [@albertchib93]. 

According to Albert and Chib, a t-distribution with a degrees of freedom of 8 is a fairly close approximation of the logistic regression model, once a correction factor of 0.634 is taken into account [@albertchib93].

To implement this link function, we must introduce a second set of latent variables, $\lambda_i$. This variable is introduced to simulate the heavier tales of the t-distribution compared to the normal distribution. Our implementation is slightly different from Albert and Chib's because of the ability to sample from a truncated t distribution and issues implementing their algorithm. We still need to simulate $\lambda$ in order to sample from the conditional posterior of $\beta^{(k)}|\vec Z, \vec Y, \lambda, \nu$. We took the degrees of freedom equal to 8 to campare results to the logistic regression model. You could sample from the posterior distribution of $\nu$ and better understand how well your model fits the data, but we choose to focus on logistic regression. 


### Algorithm:

\begin{algorithm}[H]
\caption{Tobit Regression Using Gibbs Sampler with Data Augmentation ($\nu =8$)}
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\vec{Y}$,$\textbf{X}$}
\Output{Posterior Samples of Regression Coeffiecents, $\vec \beta$}
\BlankLine
Set Number of Samples (Total ($N_s$) and Burn-in) \\
n = Number of Observations \\ 
Initailize $\vec \beta^{(0)}$ and set $\lambda = \vec 1_n$ \\
$\nu=8$
\BlankLine
\For{k = 1 to $N_s$}{
  \For{i = 1 to n}{
    \eIf{$y_i$ = 1}{
        Sample $z_i^{(k)}$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,$\lambda_i^{-1}$,0,$\infty$)\;
    }{
        Sample $z_i^{(k)}$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,$\lambda_i^{-1}$,$-\infty$,0)\;
    }
  }
  W = diag($\vec \lambda$) \\
  Set $\Sigma = (X^TWX)^{-1}$ \\
  $\vec \beta_Z = \Sigma X^TWZ$ \\
  \BlankLine
  Sample $\beta^{(k)}|\vec Z,\vec \lambda,\vec Y \nu$ from $\mathcal{N}(\hat{\vec \beta_Z}^{(k)}, \textbf{$\Sigma$})$ \\
  Sample $\lambda_i$ from $\Gamma\left(\frac{\nu+1}{2},\frac{\nu+(Z_i-x_i^T\beta)^2}{2}\right)$
}
\end{algorithm} 

## Multinomial

The previous two data augmentation Gibbs samplers tackled regression models where the response is binary. Data augmentation Gibbs samplers can also be applied to multinomial response variables. Although there are approaches for both ordered and unordered categories, we will focus on the case of ordered categories. Once again, using a probit model will simplify the conditional posterior distributions. Before discussing the alogrithm in depth, we present the ordered multinomial probit [@aitchison1957;@gurland1960;@mccullagh1980] model for which we will use Gibbs Sampling to fit.


\begin{equation}
Y_i|\vec x_i, \beta \sim Multinomial(p_{ij})
\end{equation}
for $i = 1,..., n$ and $j = 1,..., J-1$.

\begin{equation}
p_{ij} = \Phi(\gamma_j - \vec x_i^T \vec \beta)
\end{equation}

\begin{equation}
\vec \gamma, \vec \beta \sim \pi(\vec \beta, \vec \gamma)
\end{equation}

where $\pi(\vec \beta, \vec \gamma)$ is the prior on the regression coeffeicents. We will assume to be non-informative as is commonly done in analysis. 

Similar to the binary case, a latent variable $\vec Z = (Z_1,...Z_n)$ is introduced making it possible to simulate from a joint posterior. Further, we can marginalize over $\vec Z$ to find the posterior distribution of $\beta|\vec{Z},\vec{Y},\vec \gamma$, which is the same multivarite normal distribution as the binary case, and the conditional posterior of $\vec \gamma|\vec{Z},\vec{Y}, \vec \beta$. This Gibbs sampler requires only sampling from truncated normal distributions, a multivarite distribution, and a unifrom distribution. [@albertchib93] The algorithm below presents, in detail, the sampling mechanism of this data augmentation Gibbs Sampler. 


### Algorithm:


\begin{algorithm}[H]
\caption{Ordered Multinomial Probit Regression Using Gibbs Sampler with Data Augmentation}
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\vec{Y}$,$\textbf{X}$}
\Output{Posterior Samples of Regression Coeffiecents, $\vec \beta$ and $\vec \gamma$}
\BlankLine
Set Number of Samples (Total ($N_s$) and Burn-in) \\
n = Number of Observations \\ 
Initailize $\vec \beta^{(0)}$ and $\vec \gamma^{(0)}$ as the MLE \\

\BlankLine
\For{k = 1 to $N_s$}{
  Sample $\vec \gamma_j$ from $Uniform(max(max(Z_i:Y_i = j),\gamma_{j-1}^{(k-1)},min(min(Z_i:Y_i = j+1),\gamma_{j+1}^{(k-1)}))$
  
  \For{i = 1 to n}{
        Sample $z_i^{(k)}|\vec \beta,\vec \gamma, y_i=j$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,1,$\gamma_{j-1},\gamma_j$)
    }
  Set $\Sigma = (X^TX)^{-1}$ \\
  Set $\vec \beta_Z = \Sigma X^TZ$ \\
  \BlankLine
    Sample $\beta^{(k)}|\vec Z,\vec \gamma,\vec Y$ from $\mathcal{N}(\hat{\vec \beta_Z}^{(k)}, \textbf{$\Sigma$})$
}
\end{algorithm}


## Implementation in Stan (NUTS)

While not the main focus of this paper, we decided to implement the previously described models in a modern MCMC method, a No-U-Turn Sampler (NUTS). NUTS are an extension of Hamiltonian Monte Carlo. We used Stan (through the `rstan` package in R) to implent this models using NUTS. Simple Stan models are fairly straight foward to fit, making it ideal for comaprison with our data-agumantation Gibbs Sampler.   

# Results
<!-- (applying your algorithm to simulated or real data) -->

For three data sets, we apply the appropriate previously discussed model as well as standard maximum likelihood estimation and custom models written in Stan [@mcstan] and implemented in the RStan package [@rstan].

## Small-Cell Carcinoma

The small-cell carcinoma data comes from our STAT 211 class. Small-cell carcinoma of the lung is an aggressive cancer that can be treated with chemotherapy. Patients with small-cell carcinoma were randomly assigned to one of two therapy options. The outcome of the therapy is recorded on an ordinal scale (1: Progressive, 2: No Change, 3: Partial Remission, 4: Complete Remission). We fit multinomial ordered probit regression models with  with therapy option and sex as predictors of outcome. The model parameters are estimated with data augmented Gibbs sampling following the algorithm described in @albertchib93 and implemented in R, a custom model built in Stan, and traditional maximum likelihood estimation as implemented in the `polr` function in the MASS package [@MASS]. Both Bayesian methods are run to generate 4000 posterior samples, with the first 1000 discarded as burn-in samples, leaving 3000 samples for analysis. The Stan model took 1 minute and 13,9 seconds seconds to run, while the R model completed in 2.4 seconds.
<!-- 73.8619 -->

```{r Ordered Multinomial R}
ssc_gibbs_post <- read_rds(here("Small Cell Carcinoma", "ordered_multinomial.rds"))
post_sample_summary(ssc_gibbs_post) %>% 
  kable(caption = "Posterior sample summaries for small-cell carcinoma data using R-Gibbs")

trace_plot(ssc_gibbs_post) + ggtitle("Posterior Trace (Small-Cell Carcinoma Using R-Gibbs)")
dist_plot(ssc_gibbs_post) + ggtitle("Posterior Distribution (Small-Cell Carcinoma Using R-Gibbs)")
```

```{r Ordered Multinomial Stan}
multinomial_stan_post <- read_rds(here("Small Cell Carcinoma", "stan_samples_mp.rds")) %>% 
  as.matrix() %>% 
  as_tibble() %>% 
  select(-lp__) %>% 
  `colnames<-`(names(ssc_gibbs_post))
  
post_sample_summary(multinomial_stan_post) %>%
  kable(caption = "Posterior sample summaries for small-cell carcinoma data using Stan")

trace_plot(multinomial_stan_post) + ggtitle("Posterior Trace (Small-Cell Carcinoma Using Stan)")
dist_plot(multinomial_stan_post) + ggtitle("Posterior Distribution (Small-Cell Carcinoma Using Stan)")
```

```{r Ordered Multinomial MLE}
scc_dat <- read.table(here("Small Cell Carcinoma", "carcin.txt")) %>% 
  as_tibble() %>% 
  mutate(outcome = as_factor(outcome)) %>%
  uncount(count)

multinomial_mle_est <- MASS::polr(factor(outcome) ~ female + treatment, data = scc_dat, method = "probit")

suppressMessages(summary(multinomial_mle_est)) %>% 
  `$`(coefficients) %>% 
  as.data.frame() %>% 
  rownames_to_column("Variable") %>% 
  mutate(Variable = names(ssc_gibbs_post)) %>% 
  select(-"t value") %>% 
  rename(Estimate = Value) %>% 
  kable(caption = "MLE estimates for small-cell carcinoma data")
```

For this set of data, the results from the three models are fairly simliar. The Gibbs Sampler and the Stan models are slightly different (more so than the succeding data sets) from eachother for the three distributions on $\gamma$'s. This is probably to issues of mixing for the data augmented Gibbs Sampler. The Traceplots are show above do not provide evidence of mixing. We can also see strange shaped posterior distrbutions. This issues will be further discussed in Section 5. 

## Breast Cancer Data

### Probit Regression

We evaluate the probit regression methods with the Wisconsin Breast Cancer dataset, obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg [@wolberg1990multisurface]. The data reports 9 discrete measurements for 699 observations of clumps of breast cancer cells as well as the response variable, indicating whether or not the clump is malignant (1) or benign (0). The variables are Clump Thickness, Uniformity of Cell Size, Uniformity of Cell Shape, Marginal Adhesion, Single Epithelial Cell Size, Bare Nuclei, Bland Chromatin, Normal Nucleoli, and Mitoses. Only the Bare Nuclei variable included any missing data, so it was not included in this analysis. The model parameters are estimated with data-augmented Gibbs sampling following the algorithm described in @albertchib93 and implemented in R, a custom model built in Stan, and traditional maximum likelihood estimation as implemented in the `glm` function in R [@Rstats]. Both Bayesian methods are run to generate 4000 posterior samples, with the first 1000 discarded as burn-in samples, leaving 3000 samples for analysis. The Stan model took 6 minutes and 10.5 seconds seconds to run, while the R model completed in 7.8 seconds.

<!-- 310.555 -->

```{r Breast Cancer R}
bc_gibbs_post <- read_rds(here("Breast Cancer", "gibbs_samples_bc.rds"))
post_sample_summary(bc_gibbs_post) %>% 
  kable(caption = "Posterior sample summaries for breast cancer data using R-Gibbs")

trace_plot(bc_gibbs_post) + ggtitle("Posterior Trace (Breast Cancer Using R-Gibbs)")
dist_plot(bc_gibbs_post) + ggtitle("Posterior Distribution (Breast Cancer Using R-Gibbs)")
```


```{r Breast Cancer Stan}
bc_stan_post <- read_rds(here("Breast Cancer", "stan_samples_bc.rds")) %>% 
  as.matrix() %>% 
  as_tibble() %>% 
  select(-lp__) %>% 
  `colnames<-`(names(bc_gibbs_post))
  
post_sample_summary(bc_stan_post) %>%
  kable(caption = "Posterior sample summaries for breast cancer data using Stan")

trace_plot(bc_stan_post) + ggtitle("Posterior Trace (Breast Cancer Using Stan)")
dist_plot(bc_stan_post) + ggtitle("Posterior Distribution (Breast Cancer Using Stan)")
```



```{r Breast Cancer MLE, warning=FALSE}
dat_bc <- read_csv(here("Breast Cancer", "breast-cancer-wisconsin.data"),
                col_names = c('Sample code number',
                              'Clump Thickness',
                              'Uniformity of Cell Size',
                              'Uniformity of Cell Shape',
                              'Marginal Adhesion',
                              'Single Epithelial Cell Size',
                              'Bare Nuclei',
                              'Bland Chromatin',
                              'Normal Nucleoli',
                              'Mitoses',
                              'Class'), na = '?', ) %>% 
  select(-'Sample code number') %>% 
  select(-"Bare Nuclei") %>% 
  mutate(Class = Class / 2 - 1)

bc_mle_est <- glm(Class ~ ., family = binomial(link = "probit"), data = dat_bc)

summary(bc_mle_est)$coefficients %>% 
  as.data.frame() %>% 
  rownames_to_column("Variable") %>% 
  as_tibble() %>% 
  select("Variable", "Estimate", "Std. Error") %>% 
  kable(caption = "MLE estimates for breast cancer data")
```

We can see that the probit regression model for this data garnered very similar results from all three methods. We can see that the Stan model mixed better than the Gibbs Sampler, but the posterior distributions are very similar. Once again, The mixing issues with the data-augmented Gibbs Sampler will be discussed in greater detail in Section 5. 

### Tobit/Logistic Regression 

We will now run the same analysis using the t-link with eight degrees of freedom and the logistic regression model run in Stan. 

## Baseball

We evaluate the performance of data-augmented Gibbs sampling on a larger datasets (more predictors and observations than the breast cancer data), by fitting probit regression models on baseball data. The baseball data comes from Major League Baseball pitch tracking software and was prepared by the Los Angeles Dodgers for the purpose of predicting whether or not a pitch was put into play. There are 100,000 pitches (observations) thrown by Dodger's starting pitchers and 19 features of each pitch, including velocity, spin rate, and location. The binary response indiactes whether or not a pitch was put into play (0: not put into play, 1: put into play). Missing data was removed, leaving 99,254 observations with 18 covariates for analysis. The model parameters are estimated with data augmented Gibbs sampling following the algorithm described in @albertchib93 and implemented in R and traditional maximum likelihood estimation as implemented in the `glm` function in R [@Rstats]. Both Bayesian methods are run to generate 5000 posterior samples, with the first 1000 discarded as burn-in samples, leaving 4000 samples for analysis. We also attempted to fit a custom model in Stan but the samples were generated very slowly (fewer than 500 samples per hour). In contrast, the R model generated all 5000 samples in 15 miutes and 41 seconds.

<!-- R: 941.138 -->
<!-- Stan: started at 5:28 -->

```{r Baseball R, warning=FALSE}
bb_gibbs_post <- read_rds(here("baseball", "baseball_samples.rds"))
post_sample_summary(bb_gibbs_post) %>% 
  kable(caption = "Posterior sample summaries for baseball data using R-Gibbs")

trace_plot(bb_gibbs_post) + ggtitle("Posterior Trace (Bawball Using R-Gibbs)")
dist_plot(bb_gibbs_post) + ggtitle("Posterior Distribution (Baseball Using R-Gibbs)")
```

```{r Baseball Stan, warning=FALSE}
# didn't happen
```

```{r Baseball MLE, warning=FALSE}
dat_bb <- read_csv(here("Baseball", "LAD_assessment_data.csv"),
                col_types = cols(batter_mlb_id = col_skip(), 
                                 pitcher_mlb_id = col_skip(), 
                                 venue_city = col_skip())) %>%
  drop_na()

bb_mle_est <- glm(is_in_play ~ ., family = binomial(link = "probit"), data = dat_bb)

summary(bb_mle_est)$coefficients %>% 
  as.data.frame() %>% 
  rownames_to_column("Variable") %>% 
  as_tibble() %>% 
  select("Variable", "Estimate", "Std. Error") %>% 
  kable(caption = "MLE estimates for baseball data")
```

<!-- There were 23 model parameters -->

As we can see the MLE results and the results from the dat-augmented Gibbs Sampler is quite similar when comparing the mean and standard deivation estimates of the two methods. We can see from the traceplots that the Gibbs Sampler mixed well. 

# Discussion
<!-- (shortcomings of this new algorithm and how it can be improved). -->

The data-augmented Gibbs sampling algorithms applied in this project largely performed well and the results were comparable to the Stan models. They were significanlty faster than the regression modesl implemented in Stan, which made analysis of the large baseball dataset feasible in a short amount of time, when it would have otherwise taken much longer. This model also mixed quite well, so infernce should be trusted with the Gibbs Sampler. The chief shortcoming of these methods is the poor mixing properties we observed with the Small-Cell Carcinoma data and the Breast Cancer data (although not as severe). Simulating much greater samples and using thinning did not lead to any improvement in mixing. Despite the poor mixing of the chains, the posterior means and medians closely aligned with the results observed in the Stan model and obtained with maximum likelihood estimation.

In addition the use of flat, improper priors may be of concern to some practioners. The data-augmented Gibbs Sampler can accommodate the use of specific priors, the only change is the ease of sampling. We do not have enough information regarding the slopes to set proper priors anyway. The use of improper simplified the sampling process, which is why they were used in this paper and are commonly used for regression. We even used improper priors in the Stan model.

Although the models described in @albertchib93 and demonstrated in this project remain popular to this day [@chakraborty2017], much works has been done to expand upon its foundation. Several papers have made efforts to apply the strategy described here to the logit model [@holmes2006; @Schnatter2010; @gramacy2012; @Polson2013] as wells as negative binomial regression [@NIPS2012_4567].

Current research is being done to analyze baseball data as it pertains to pitcher changes and in-game decision making [@baseball1]. A hierarchical probit model is being used to estimate spline functions of how pitchers "age" while playing in a Major League Baseball game. This model was adapted from @baseball, which looked at players actually aging over their carrers, but uses a probit link instead of a logit link function. The probit link is being used in order to use data-augmented Gibbs Sampling for the sampling of the posterior distributions and for parameter estimation on the spline functions.

*NEED AN END PARAGRAPH!!!!*




\pagebreak

# References

::: {#refs}
:::

\pagebreak

# Appendix