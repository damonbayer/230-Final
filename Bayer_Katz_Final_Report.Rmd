---
title: "Gibbs Sampling with Data Augmentation for Bayesian Analysis of Binary and Polychotomous Response Data"
subtitle: "STAT 230 Final Report"
author: "Damon Bayer & Corey Katz"
date: "12/16/2019"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    df_print: kable
header-includes:
  - \usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
  - \usepackage{amsmath}
  - \usepackage{bm}
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, cache=T)
library(tidyverse)
library(here)
library(knitr)
source(here("plotting_functions.R"))

options(readr.num_columns = 0)

post_sample_summary <- function(post) {
  post %>% 
  pivot_longer(everything()) %>%
  mutate(name = as_factor(name)) %>% 
  rename(Variable = name) %>% 
  group_by(Variable) %>% 
  summarize_all(list(mean = mean,
                     sd = sd,
                     "2.5%" = function(x) quantile(x, probs = 0.025),
                     "50%" = median,
                     "97.5%" = function(x) quantile(x, probs = 0.975)))
}
```

# Abstract

This project focuses on implementations and applications of several of the algorithms presented in @albertchib93, including data-augmented Gibbs sampling for probit regression on binary response data, multinomial probit regression on ordinal response data, and t-link regression on binary response data. We compare these method to the equivalent MLE estimates, as well as a more modern MCMC method, No-U-Turn Samplers, as implemented in Stan. Broadly, these methods all resulted in similar point estimates for regression parameters. The data-augmented Gibbs sampling performed significantly faster than the Stan models, but had some issues with mixing in the posterior distributions. Further research has been done to apply the methods here to regression methods on binary and polychotomous response data with other link functions. The authors hope to apply some of the concepts developed here in an ongoing research project to model pitching change decisions in baseball.

# Introduction

Markov Chain Monte Carlo (MCMC) methods revolutionize Bayesian inference, making it possible to sample from complex posterior distributions. As computing power continues to evolve, so do sampling methods, as well as the complexity of models scientists try to fit. While advances have been made to MCMC, sampling from non-conjugate posterior distributions is still a difficult and time-consuming task. Gibbs sampling is one of the most common forms of MCMC and one of the earliest attempts to alleviate these struggles of sampling from intractable posteriors was the development of data-augmentation Gibbs Sampling for regression models. 

@albertchib93 was the first paper to introduce using data augmentation to fit Bayesian probit regression models to binary and  polychotomous data.  It builds upon the foundation of Gibbs sampling in  @gelfand1990sampling and @tannerwong1987. Adding latent variables has been used in many other areas of statistics and mathematics to aid in computation. By introducing a latent variable to the probit regression model of a binary response variable, they are able to simplify the Gibbs sampling process and make sampling from the posterior distribution easier. This paper laid the groundwork for using data-augmentation Gibbs Sampling for Bayesian inference by enabling the computation of exact posterior distributions of regression coefficients for binary and polychotomous response.

Compared to traditional maximum likelihood estimation (MLE), Albert and Chibâ€™s approach is advantageous in the case of small data, where MLE can be biased, as well as in the case of regression models with complicated likelihoods, such as in the multivariate probit case. Additionally, this method enables the modeling of the marginal distribution of residuals, which are on a continuous scale and can be more helpful for outlier detection than frequentist residuals, which only take on two values.

This paper focuses on implementations and applications of several of the algorithms presented in @albertchib93, including the aforementioned data-augmented Gibbs sampling for probit regression on binary response data, multinomial probit regression on ordinal response data, and t-link regression on binary response data. We compare these method to the equivalent MLE estimates (Generalized Linear Models), as well as a more modern MCMC method, No-U-Turn Samplers, as implemented in Stan [@mcstan]. By comparing these three methods, we can better understand the importance of the introduction of data augmentation to Bayesian inference.


# Methodology
<!-- (explaining the algorithm), -->

In this section, we detail each method for data-augmented Gibbs sampling, including pseudocode for each algorithm.

## Probit Model 

The first model that data augmentation can be used to find posterior distributions is the probit model. The following is a standard Bayesian probit model with non-informative priors on the regression coefficients: 

\begin{equation}
y_i| \pi_{i},\vec{\beta}, \vec x_i \sim Bernoulli(\pi_{i})  \ \ \ \text{for} \ \  i = 0,...,n
\end{equation}

\begin{equation}
\pi_{i} = \Phi(x_i^T\beta)
\end{equation}

\begin{equation}
\beta_j \sim N(0, 100),  \ \ \ \text{for} \  j = 0,...,p
\end{equation}


Where $\Phi$ is the cumulative density function of the normal distribution. 

The idea of data augmentation is to introduce a continuous latent variable derived from a binary response in order to make sampling from the posterior distributions of the probit model coefficients easier. By introducing independent latent variables $Z_1, ... Z_n$, we can now find the joint posterior of $\vec \beta$ and $\vec Z$ given Y. We can then marginalize over the posterior distribution of $\vec Z|\vec Y$ and thus we have the conditional posterior of $\beta|\vec{Z},\vec{Y}$. This method is further simplified if we assume (as we did above) non-informative priors on the regression coefficients, $\vec \beta$ [@albertchib93]. 

With this approach we are able to create a Gibbs sampler that only needs to sample from truncated normal distributions and multivariate normal distribution, which are extremely easy with today's computing power. Based on the fully conditional posterior of $\beta|\vec{Z},\vec{Y}$, Albert and Chib equated this method of probit regression on binary Y to doing linear regression on the latent variable Z. This will be evident in the algorithm as we use the `lm` function to find the mean of the posterior distribution of $\beta|\vec{Z},\vec{Y}$ [@albertchib93]. 

### Algorithm: 

\begin{algorithm}[H]
\caption{Probit Regression Using Gibbs Sampler with Data Augmentation}
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\vec{Y}$,$\textbf{X}$}
\Output{Posterior Samples of Regression Coefficients, $\vec \beta$}
\BlankLine
Set Number of Samples (Total ($N_s$) and Burn-in) 
Initialize $\vec \beta^{(0)}$ \\
Set $\Sigma = (X^TX)^{-1}$
n = Number of Observations
\BlankLine
\For{k = 1 to $N_s$}{
  \For{i = 1 to n}{
    \eIf{$y_i$ = 1}{
        Sample $z_i^{(k)}$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,1,0,$\infty$)\;
    }{
        Sample $z_i^{(k)}$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,1,$-\infty$,0)\;
    }
  }
  Regress $\vec Z$ onto $\textbf{X}$ to find $\vec \beta_Z$ \\
  Sample $\beta^{(k)}|\vec Z$ from $\mathcal{N}(\hat{\vec \beta}_Z^{(k)}, \bm{\Sigma})$
  
  
}
\end{algorithm}
  
## T-link to Approximate Logistic Regression

In this section we discuss an extension of the Gibbs sampler discussed for the probit model. The purpose of this model is to use data augmentation with t-distributions to approximate the logistic regression model for a binary response. The simple Bayesian Logistic Regression model with non-informative priors is as follows: 

\begin{equation}
y_i| \pi_{i},\vec{\beta}, \vec x_i \sim Bernoulli(\pi_{i})  \ \ \ \text{for} \ \  i = 0,...,n
\end{equation}

\begin{equation}
\text{logit}(\pi_{i}) = x_i^T\beta
\end{equation}

\begin{equation}
\beta_j \sim N(0, 100),  \ \ \ \text{for} \  j = 0,...,p
\end{equation}

Where $\text{logit}(p) = \frac{p}{1-p}$.

Rather than using the normal cdf (as in the probit model) we use the t-distribution cdf as our link function. Replacing $\Phi$ with the cdf of the t($\nu$) in (2), we achieve the Bayesian t-link model. By generalizing the model, we can now choose a degrees of freedom, $\nu$, where t($\nu$) better fits out model and thus a link function that approximates other well-known link functions. For example, many practitioners prefer logistic regression because of the interpretability of the coefficients as odds ratios. The flexibility of the t-link function allows us to draw from a posterior distribution of $\vec \beta$ that is approximately the posterior distribution if we had chosen to use the logit link function. Note that if we set the degrees of freedom equal to infinity, we would revert back to the probit model [@albertchib93].

A t-distribution with a degrees of freedom of 8 is a fairly close approximation of the logistic regression model, once a correction factor of 0.634 is taken into account [@albertchib93].

To implement this link function, we introduce a second set of latent variables, $\lambda_i$. This variable is introduced to simulate the heavier tales of the t-distribution compared to the normal distribution. Our implementation is slightly different from Albert and Chib's, due to some issues we encountered with their exact implementation. Albert and Chib describe a procedure for evaluating different degrees of freedom in the t-link in order to choose the best fit to the data, but we were unable to successfully implement it for this project. Because of this, we choose to focus only on 8 degrees of freedom to enable comparisons to logistic regression.


### Algorithm:

\begin{algorithm}[H]
\caption{Tobit Regression Using Gibbs Sampler with Data Augmentation ($\nu =8$)}
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\vec{Y}$,$\textbf{X}$}
\Output{Posterior Samples of Regression Coefficients, $\vec \beta$}
\BlankLine
Set Number of Samples (Total ($N_s$) and Burn-in) \\
n = Number of Observations \\ 
Initialize $\vec \beta^{(0)}$ and set $\lambda = \vec 1_n$ \\
$\nu=8$
\BlankLine
\For{k = 1 to $N_s$}{
  \For{i = 1 to n}{
    \eIf{$y_i$ = 1}{
        Sample $z_i^{(k)}$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,$\lambda_i^{-1}$,0,$\infty$)\;
    }{
        Sample $z_i^{(k)}$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,$\lambda_i^{-1}$,$-\infty$,0)\;
    }
  }
  W = diag($\vec \lambda$) \\
  Set $\Sigma = (X^TWX)^{-1}$ \\
  $\vec \beta_Z = \Sigma X^TWZ$ \\
  \BlankLine
  Sample $\beta^{(k)}|\vec Z,\vec \lambda,\vec Y \nu$ from $\mathcal{N}(\hat{\vec \beta_Z}^{(k)}, \textbf{$\Sigma$})$ \\
  Sample $\lambda_i$ from $\Gamma\left(\frac{\nu+1}{2},\frac{\nu+(Z_i-x_i^T\beta)^2}{2}\right)$
}
\end{algorithm} 

## Multinomial

Data augmentation Gibbs samplers can also be applied to multinomial response variables. Although there are approaches for both ordered and unordered categories, we will focus on the case of ordered categories. Once again, using a probit model will simplify the conditional posterior distributions. Before discussing the algorithm in depth, we present the ordered multinomial probit [@aitchison1957;@gurland1960;@mccullagh1980] model for which we will use Gibbs Sampling to fit.


\begin{equation}
Y_i|\vec x_i, \beta \sim Multinomial(p_{ij})
\end{equation}
for $i = 1,..., n$ and $j = 1,..., J-1$.

\begin{equation}
p_{ij} = \Phi(\gamma_j - \vec x_i^T \vec \beta)
\end{equation}

\begin{equation}
\vec \gamma, \vec \beta \sim \pi(\vec \beta, \vec \gamma)
\end{equation}

where $\pi(\vec \beta, \vec \gamma)$ is the prior on the regression coefficients. We will assume to be non-informative as is commonly done in analysis. 

Similar to the binary case, a latent variable $\vec Z = (Z_1,...Z_n)$ is introduced making it possible to simulate from a joint posterior. Further, we can marginalize over $\vec Z$ to find the posterior distribution of $\beta|\vec{Z},\vec{Y},\vec \gamma$, which is the same multivariate normal distribution as the binary case, and the conditional posterior of $\vec \gamma|\vec{Z},\vec{Y}, \vec \beta$. This Gibbs sampler requires only sampling from truncated normal distributions, a multivariate distribution, and a uniform distribution [@albertchib93]. The algorithm below presents, in detail, the sampling mechanism of this data augmentation Gibbs sampler. 


### Algorithm:


\begin{algorithm}[H]
\caption{Ordered Multinomial Probit Regression Using Gibbs Sampler with Data Augmentation}
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\vec{Y}$,$\textbf{X}$}
\Output{Posterior Samples of Regression Coefficients, $\vec \beta$ and $\vec \gamma$}
\BlankLine
Set Number of Samples (Total ($N_s$) and Burn-in) \\
n = Number of Observations \\ 
Initialize $\vec \beta^{(0)}$ and $\vec \gamma^{(0)}$ as the MLE \\

\BlankLine
\For{k = 1 to $N_s$}{
  Sample $\vec \gamma_j$ from $Uniform(max(max(Z_i:Y_i = j),\gamma_{j-1}^{(k-1)},min(min(Z_i:Y_i = j+1),\gamma_{j+1}^{(k-1)}))$
  
  \For{i = 1 to n}{
        Sample $z_i^{(k)}|\vec \beta,\vec \gamma, y_i=j$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,1,$\gamma_{j-1},\gamma_j$)
    }
  Set $\Sigma = (X^TX)^{-1}$ \\
  Set $\vec \beta_Z = \Sigma X^TZ$ \\
  \BlankLine
    Sample $\beta^{(k)}|\vec Z,\vec \gamma,\vec Y$ from $\mathcal{N}(\hat{\vec \beta_Z}^{(k)}, \textbf{$\Sigma$})$
}
\end{algorithm}


## Implementation in Stan (NUTS)

While not the main focus of this paper, we decided to implement the previously described models in a modern MCMC method, a No-U-Turn Sampler (NUTS). NUTS are an extension of Hamiltonian Monte Carlo. We used Stan (through the `Rstan` package in R [@mcstan; @rstan]) to implement this models using NUTS.   

# Results
<!-- (applying your algorithm to simulated or real data) -->

For three data sets, we apply the appropriate previously discussed model as well as standard maximum likelihood estimation and custom models written in Stan [@mcstan] and implemented in the RStan package [@rstan].

## Small-Cell Carcinoma

The small-cell carcinoma data comes from our STAT 211 class. Small-cell carcinoma of the lung is an aggressive cancer that can be treated with chemotherapy. Patients with small-cell carcinoma were randomly assigned to one of two therapy options. The outcome of the therapy is recorded on an ordinal scale (1: Progressive, 2: No Change, 3: Partial Remission, 4: Complete Remission). We fit multinomial ordered probit regression models with  with therapy option and sex as predictors of outcome. The model parameters are estimated with data augmented Gibbs sampling following the algorithm described in @albertchib93 and implemented in R, a custom model built in Stan, and traditional maximum likelihood estimation as implemented in the `polr` function in the MASS package [@MASS]. Both Bayesian methods are run to generate 4000 posterior samples, with the first 1000 discarded as burn-in samples, leaving 3000 samples for analysis. The Stan model took 1 minute and 13.9 seconds seconds to run, while the R model completed in 2.4 seconds.
<!-- 73.8619 -->

```{r Ordered Multinomial R}
ssc_gibbs_post <- read_rds(here("Small Cell Carcinoma", "ordered_multinomial.rds"))
post_sample_summary(ssc_gibbs_post) %>% 
  kable(caption = "Posterior sample summaries for small-cell carcinoma data using R-Gibbs")

trace_plot(ssc_gibbs_post) + ggtitle("Posterior Trace (Small-Cell Carcinoma Using R-Gibbs)")
dist_plot(ssc_gibbs_post) + ggtitle("Posterior Distribution (Small-Cell Carcinoma Using R-Gibbs)")
```

```{r Ordered Multinomial Stan}
multinomial_stan_post <- read_rds(here("Small Cell Carcinoma", "stan_samples_mp.rds")) %>% 
  as.matrix() %>% 
  as_tibble() %>% 
  select(-lp__) %>% 
  `colnames<-`(names(ssc_gibbs_post))
  
post_sample_summary(multinomial_stan_post) %>%
  kable(caption = "Posterior sample summaries for small-cell carcinoma data using Stan")

trace_plot(multinomial_stan_post) + ggtitle("Posterior Trace (Small-Cell Carcinoma Using Stan)")
dist_plot(multinomial_stan_post) + ggtitle("Posterior Distribution (Small-Cell Carcinoma Using Stan)")
```

```{r Ordered Multinomial MLE}
scc_dat <- read.table(here("Small Cell Carcinoma", "carcin.txt")) %>% 
  as_tibble() %>% 
  mutate(outcome = as_factor(outcome)) %>%
  uncount(count)

multinomial_mle_est <- MASS::polr(factor(outcome) ~ female + treatment, data = scc_dat, method = "probit")

suppressMessages(summary(multinomial_mle_est)) %>% 
  `$`(coefficients) %>% 
  as.data.frame() %>% 
  rownames_to_column("Variable") %>% 
  mutate(Variable = names(ssc_gibbs_post)) %>% 
  select(-"t value") %>% 
  rename(Estimate = Value) %>% 
  kable(caption = "MLE estimates for small-cell carcinoma data")
```

For this set of data, the results from the three models are fairly similar. The trace plots for the Gibbs sampler show a large degree of auto-correlation for the $gamma$ variables, which is a likely explanation for the difference between the Gibbs and Stan estimates. We also note asymmetric posterior distributions. This issues will be further discussed in Section 5. 

## Breast Cancer

### Probit Regression

We evaluate the probit regression methods with the Wisconsin Breast Cancer dataset, obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg [@wolberg1990multisurface]. The data reports 9 discrete measurements for 699 observations of clumps of breast cancer cells as well as the response variable, indicating whether or not the clump is malignant (1) or benign (0). The variables are Clump Thickness, Uniformity of Cell Size, Uniformity of Cell Shape, Marginal Adhesion, Single Epithelial Cell Size, Bare Nuclei, Bland Chromatin, Normal Nucleoli, and Mitoses. Only the Bare Nuclei variable included any missing data, so it was not included in this analysis. The model parameters are estimated with data-augmented Gibbs sampling following the algorithm described in @albertchib93 and implemented in R, a custom model built in Stan, and traditional maximum likelihood estimation as implemented in the `glm` function in R [@Rstats]. Both Bayesian methods are run to generate 4000 posterior samples, with the first 1000 discarded as burn-in samples, leaving 3000 samples for analysis. The Stan model took 6 minutes and 10.5 seconds seconds to run, while the R model completed in 7.8 seconds.

<!-- 310.555 -->

```{r Breast Cancer R Probit}
bc_gibbs_post <- read_rds(here("Breast Cancer", "gibbs_samples_bc.rds"))
post_sample_summary(bc_gibbs_post) %>% 
  kable(caption = "Posterior sample summaries for breast cancer data using R-Gibbs and probit link")

trace_plot(bc_gibbs_post) + ggtitle("Posterior Trace (Breast Cancer Using R-Gibbs and probit link)")
dist_plot(bc_gibbs_post) + ggtitle("Posterior Distribution (Breast Cancer Using R-Gibbs and probit link)")
```


```{r Breast Cancer Stan Probit}
bc_stan_post <- read_rds(here("Breast Cancer", "stan_samples_bc.rds")) %>% 
  as.matrix() %>% 
  as_tibble() %>% 
  select(-lp__) %>% 
  `colnames<-`(names(bc_gibbs_post))
  
post_sample_summary(bc_stan_post) %>%
  kable(caption = "Posterior sample summaries for breast cancer data using Stan and probit link")

trace_plot(bc_stan_post) + ggtitle("Posterior Trace (Breast Cancer Using Stan and probit link)")
dist_plot(bc_stan_post) + ggtitle("Posterior Distribution (Breast Cancer Using Stan and probit link)")
```



```{r Breast Cancer MLE Probit, warning=FALSE}
dat_bc <- read_csv(here("Breast Cancer", "breast-cancer-wisconsin.data"),
                col_names = c('Sample code number',
                              'Clump Thickness',
                              'Uniformity of Cell Size',
                              'Uniformity of Cell Shape',
                              'Marginal Adhesion',
                              'Single Epithelial Cell Size',
                              'Bare Nuclei',
                              'Bland Chromatin',
                              'Normal Nucleoli',
                              'Mitoses',
                              'Class'), na = '?', ) %>% 
  select(-'Sample code number') %>% 
  select(-"Bare Nuclei") %>% 
  mutate(Class = Class / 2 - 1)

bc_mle_est <- glm(Class ~ ., family = binomial(link = "probit"), data = dat_bc)

summary(bc_mle_est)$coefficients %>% 
  as.data.frame() %>% 
  rownames_to_column("Variable") %>% 
  as_tibble() %>% 
  select("Variable", "Estimate", "Std. Error") %>% 
  kable(caption = "MLE estimates for breast cancer data and probit link")
```

We note that the probit regression model for this data garnered very similar results from all three methods. While the Stan model mixed better than the Gibbs Sampler, the posterior distributions are very similar. Mixing issues are discussed in more detail in Section 5. 

### T-Link/Logistic Regression 

We now run the same analysis using the t-link with eight degrees of freedom, the logistic regression model run in Stan and traditional maximum likelihood estimation as implemented in the `glm` function in R [@Rstats]. Each sample from the t-link model is divided by 0.634 to make comparisons to the logistic models. Both Bayesian methods are run to generate 6000 posterior samples, with the first 2000 discarded as burn-in samples, leaving 4000 samples for analysis. The Stan model took 6 minutes and 55.7 seconds seconds to run, while the R model completed in 49.3 seconds.

```{r Breast Cancer R t-link}
bc_gibbs_post_logit <- read_rds(here("Breast Cancer", "gibbs_logit_bc.rds")) / 0.634
post_sample_summary(bc_gibbs_post_logit) %>% 
  kable(caption = "Posterior sample summaries for breast cancer data using R-Gibbs and t-link")

trace_plot(bc_gibbs_post_logit) + ggtitle("Posterior Trace (Breast Cancer Using R-Gibbs  and t-link)")
dist_plot(bc_gibbs_post_logit) + ggtitle("Posterior Distribution (Breast Cancer Using R-Gibbs  and t-link)")
```


```{r Breast Cancer Stan t-link}
bc_stan_post_logit <- read_rds(here("Breast Cancer", "stan_samples_logit.rds")) %>% 
  as.matrix() %>% 
  as_tibble() %>% 
  select(-lp__) %>% 
  `colnames<-`(names(bc_gibbs_post))
  
post_sample_summary(bc_stan_post_logit) %>%
  kable(caption = "Posterior sample summaries for breast cancer data using Stan and logit link")

trace_plot(bc_stan_post_logit) + ggtitle("Posterior Trace (Breast Cancer Using Stan and logit link)")
dist_plot(bc_stan_post_logit) + ggtitle("Posterior Distribution (Breast Cancer Using Stan and logit link)")
```



```{r Breast Cancer MLE t-link,  warning=FALSE}
bc_mle_est_logit <- glm(Class ~ ., family = binomial, data = dat_bc)

summary(bc_mle_est_logit)$coefficients %>% 
  as.data.frame() %>% 
  rownames_to_column("Variable") %>% 
  as_tibble() %>% 
  select("Variable", "Estimate", "Std. Error") %>% 
  kable(caption = "MLE estimates for breast cancer data and logit link")
```

We note that the transformed t-link point estimates closely match the Stan model coefficients as well as the maximum likelihood estimates. The Gibbs sample trace plot exhibit strong auto-correlation in some plots, but the posterior distributions look fairly normal. There are no obvious problems with the posterior distributions generated with the Stan model.

## Baseball

We evaluate the performance of data-augmented Gibbs sampling on a larger datasets (more predictors and observations than the breast cancer data), by fitting probit regression models on baseball data. The baseball data comes from Major League Baseball pitch tracking software and was prepared by the Los Angeles Dodgers for the purpose of predicting whether or not a pitch was put into play. There are 100,000 pitches (observations) thrown by Dodger's starting pitchers and 19 features of each pitch, including velocity, spin rate, and location. The binary response indicates whether or not a pitch was put into play (0: not put into play, 1: put into play). Missing data was removed, leaving 99,254 observations with 18 covariates for analysis. The model parameters are estimated with data augmented Gibbs sampling following the algorithm described in @albertchib93 and implemented in R and traditional maximum likelihood estimation as implemented in the `glm` function in R [@Rstats]. Both Bayesian methods are run to generate 5000 posterior samples, with the first 1000 discarded as burn-in samples, leaving 4000 samples for analysis. We also attempted to fit a custom model in Stan but the samples were generated very slowly (fewer than 500 samples per hour). In contrast, the R model generated all 5000 samples in 15 minutes and 41 seconds.

<!-- R: 941.138 -->
<!-- Stan: started at 5:28 -->

```{r Baseball R, warning=FALSE}
bb_gibbs_post <- read_rds(here("baseball", "baseball_samples.rds"))
post_sample_summary(bb_gibbs_post) %>% 
  kable(caption = "Posterior sample summaries for baseball data using R-Gibbs")

trace_plot(bb_gibbs_post) + ggtitle("Posterior Trace (Baseball Using R-Gibbs)")
dist_plot(bb_gibbs_post) + ggtitle("Posterior Distribution (Baseball Using R-Gibbs)")
```

```{r Baseball Stan, warning=FALSE}
# didn't happen
```

```{r Baseball MLE, warning=FALSE}
dat_bb <- read_csv(here("Baseball", "LAD_assessment_data.csv"),
                col_types = cols(batter_mlb_id = col_skip(), 
                                 pitcher_mlb_id = col_skip(), 
                                 venue_city = col_skip())) %>%
  drop_na()

bb_mle_est <- glm(is_in_play ~ ., family = binomial(link = "probit"), data = dat_bb)

summary(bb_mle_est)$coefficients %>% 
  as.data.frame() %>% 
  rownames_to_column("Variable") %>% 
  as_tibble() %>% 
  select("Variable", "Estimate", "Std. Error") %>% 
  kable(caption = "MLE estimates for baseball data")
```

<!-- There were 23 model parameters -->

We note that the MLE results and the results from the data-augmented Gibbs Sampler is quite similar when comparing the mean and standard deviation estimates of the two methods. We can see from the trace plots that the Gibbs Sampler mixed well. 

# Discussion
<!-- (shortcomings of this new algorithm and how it can be improved). -->

The data-augmented Gibbs sampling algorithms applied in this project largely performed well and achieved comparable estimates to the Stan models, while running in significantly less time. The speed improvements were particularly impressive in the case of the baseball data, which enabled us to perform a Bayesian analysis when we would have otherwise not have had sufficient time. This model also mixed quite well, so inference should be trusted with the Gibbs Sampler. The chief shortcoming of these methods is the poor mixing properties we observed with the Small-Cell Carcinoma data and the Breast Cancer data (although not as severe). Simulating much greater samples and using thinning did not lead to any improvement in mixing. Despite the poor mixing of the chains, the posterior means and medians closely aligned with the results observed in the Stan model and obtained with maximum likelihood estimation.

In addition the use of flat, improper priors may be of concern to some practitioners. The data-augmented Gibbs Sampler can accommodate the use of specific priors, but this can make the sampling more difficult. This was not a concern in our analysis, given that we are not subject matter experts in any of the applications we demonstrated here.

Although the models described in @albertchib93 and demonstrated in this project remain popular to this day [@chakraborty2017], much work has been done to expand upon its foundation. Several papers have made efforts to apply the strategy described here to the logit model [@holmes2006; @Schnatter2010; @gramacy2012; @Polson2013] as wells as negative binomial regression [@NIPS2012_4567].

Current research is being done to analyze baseball data as it pertains to pitcher changes and in-game decision making [@baseball1]. A hierarchical probit model is being used to estimate spline functions of how pitchers "age" while playing in a Major League Baseball game. This model was adapted from @baseball, which looked at players actually aging over their careers, but uses a probit link instead of a logit link function. The probit link is being used in order to use data-augmented Gibbs Sampling for the sampling of the posterior distributions and for parameter estimation on the spline functions.


\pagebreak

# References

::: {#refs}
:::

\pagebreak

# Appendix

All code for this analysis is available in our GitHub repository: https://github.com/damonbayer/230-Final