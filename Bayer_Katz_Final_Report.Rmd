---
title: "Gibbs Sampling with Data Augmentation for Bayesian Analysis of Binary and Polychotomous Response Data"
subtitle: "STAT 230 Final Report"
author: "Damon Bayer & Corey Katz"
date: "12/16/2019"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    df_print: kable
header-includes:
  - \usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
  - \usepackage{amsmath}
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, cache=T)
library(tidyverse)
library(here)
library(knitr)
source(here("plotting_functions.R"))

options(readr.num_columns = 0)

post_sample_summary <- function(post) {
  post %>% 
  pivot_longer(everything()) %>%
  mutate(name = as_factor(name)) %>% 
  rename(Variable = name) %>% 
  group_by(Variable) %>% 
  summarize_all(list(mean = mean,
                     sd = sd,
                     "2.5%" = function(x) quantile(x, probs = 0.025),
                     "50%" = median,
                     "97.5%" = function(x) quantile(x, probs = 0.975)))
}
```

# Abstract

Here's the abstract.

# Introduction
<!-- (explaining the motivation behind the new algorithm) -->

This project focuses on implementations and applications of several of the algorithms presented in @albertchib93, including data-augmentated Gibbs sampling for probit regression on binary response data, multinomial probit regression on ordinal response data, and the t-link regression on binary response data. The primary contribution of this paper was enabling the computation of exact posterior distributions of regression coefficients for binary and polychotomous response data using latent variables straightforward Gibbs sampling .

Compared to traditional maximum likelihood estimation, this approach is advantageous in the case of small data, where mle can be biased, as well as in the case of regression models with complicated likelihoods, such as in the multivariate probit case. Additionally, this method enables the modeling of the marginal distribution of residuals, which are on a continuous scale and can be more helpful for outlier detection than frequentist residuals, which only take on two values.


# Methodology
<!-- (explaining the algorithm), -->

In this section, we detail each method for data-augmented Gibbs sampling, including pseudocode for each algorithm.

## Probit Model 

The first model that data augmentation can be used to find posterior distributions is the probit model. The following is a standard Bayesian Probit model with non informative priors on the regression coefficients: 

\begin{equation}
y_i| \pi_{i},\vec{\beta}, \vec x_i \sim Bernoulli(\pi_{i})  \ \ \ \text{for} \ \  i = 0,...,n
\end{equation}

\begin{equation}
\pi_{i} = \Phi(x_i^T\beta)
\end{equation}

\begin{equation}
\beta_j \sim N(0, 100),  \ \ \ \text{for} \  j = 0,...,p
\end{equation}


Note that $\Phi$ is the cumulative density function of the normal distribution. 

The idea of data augmentation is to introduce a continuous latent variable derived from a binary response in order to make sampling from the posterior distributions of the probit model coefficients easier. By introducing independent latent variables $Z_1, ... Z_n$, we can now find the joint posterior of $\vec \beta$ and $\vec Z$ given Y. We can then marginalize over the posterior distribution of $\vec Z|\vec Y$ and thus we have the conditional posterior of $\beta|\vec{Z},\vec{Y}$. This method is further simplified if we assume (as we did above) non-informative priors on the regression coefficients, $\vec \beta$ [@albertchib93]. 

With this approach we are able to create a Gibbs sampler that only needs to sample from truncated normal distributions and multivariate normal distribution, which are extremely easy with today's computing power. Based on the fully conditonal posterior of $\beta|\vec{Z},\vec{Y}$, Albert and Chib equated this method of probit regression on binary Y to doing linear regression on the latent variable Z. This will be evident in the algortim as we use the `lm` function to find the mean of the posterior distribution of $\beta|\vec{Z},\vec{Y}$ [@albertchib93]. 

### Algorithm: 

\begin{algorithm}[H]
\caption{Probit Regression Using Gibbs Sampler with Data Augmentation}
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\vec{Y}$,$\textbf{X}$}
\Output{Posterior Samples of Regression Coeffiecents, $\vec \beta$}
\BlankLine
Set Number of Samples (Total ($N_s$) and Burn-in) 
Initailize $\vec \beta^{(0)}$ \\
Set $\Sigma = (X^TX)^{-1}$
n = Number of Observations
\BlankLine
\For{k = 1 to $N_s$}{
  \For{i = 1 to n}{
    \eIf{$y_i$ = 1}{
        Sample $z_i^{(k)}$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,1,0,$\infty$)\;
    }{
        Sample $z_i^{(k)}$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,1,$-\infty$,0)\;
    }
  }
  Regress $\vec Z$ onto $\textbf{X}$ to find $\vec \beta_Z$ \\
  Sample $\beta^{(k)}|\vec Z$ from $\mathcal{N}(\hat{\vec \beta_Z}^{(k)}, \textbf{$\Sigma$})$
  
  
}
\end{algorithm}
  
## T-link to Approximate Logistic Regression

In this section we will discuss an extension of the Gibbs sampler discussed for the probit model. The purpose of this model is to use data augmentation with t-distributions to approximate the logistic regression model for a binary response. The simple Bayesian Logistic Regression model with non-informative priors is as follows: 

\begin{equation}
y_i| \pi_{i},\vec{\beta}, \vec x_i \sim Bernoulli(\pi_{i})  \ \ \ \text{for} \ \  i = 0,...,n
\end{equation}

\begin{equation}
\text{logit}(\pi_{i}) = x_i^T\beta
\end{equation}

\begin{equation}
\beta_j \sim N(0, 100),  \ \ \ \text{for} \  j = 0,...,p
\end{equation}

Where $\text{logit}(p) = \frac{p}{1-p}$ 

The T-link is an extension of the probit regression model because instead of using the normal cdf (as in the probit model) we use the t-distribution cdf as our link function. If we replace $\Phi$ with the cdf of the t($\nu$) in (2), we would have Bayesian t-link model. By generalizing the model, we can now choose a degrees of freedom, $\nu$ where t($\nu$) better fits out model and thus a link function that approximates other well-known link functions. For example, we are usually concerned with logistic regression because of the interpretability of the coefficients into odds ratios. The flexibility of the t-link function allows us to draw from a posterior distribution of $\vec \beta$ that is approximately the posterior distribution if we had chosen to use the logit link function. Note that if we set the degrees of freedom equal to infinity, we would revert back to the probit model [@albertchib93]. 

According to Albert and Chib, a t-distribution with a degrees of freedom of 8 is a fairly close approximation of the logistic regression model, once a correction factor of 0.634 is taken into account [@albertchib93].

To implement this link function, we must introduce a second set of latent variables, $\lambda_i$. This variable is introduced to simulate the heavier tales of the t-distribution compared to the normal distribution. Our implementation is slightly different from Albert and Chib's because of the ability to sample from a truncated t distribution and issues implementing their algorithm. We still need to simulate $\lambda$ in order to sample from the conditional posterior of $\beta^{(k)}|\vec Z, \vec Y, \lambda, \nu$. We took the degrees of freedom equal to 8 to campare results to the logistic regression model. You could sample from the posterior distribution of $\nu$ and better understand how well your model fits the data, but we choose to focus on logistic regression. 


### Algorithm:

\begin{algorithm}[H]
\caption{Tobit Regression Using Gibbs Sampler with Data Augmentation ($\nu =8$)}
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\vec{Y}$,$\textbf{X}$}
\Output{Posterior Samples of Regression Coeffiecents, $\vec \beta$}
\BlankLine
Set Number of Samples (Total ($N_s$) and Burn-in) \\
n = Number of Observations \\ 
Initailize $\vec \beta^{(0)}$ and set $\lambda = \vec 1_n$ \\
$\nu=8$
\BlankLine
\For{k = 1 to $N_s$}{
  \For{i = 1 to n}{
    \eIf{$y_i$ = 1}{
        Sample $z_i^{(k)}$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,$\lambda_i^{-1}$,0,$\infty$)\;
    }{
        Sample $z_i^{(k)}$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,$\lambda_i^{-1}$,$-\infty$,0)\;
    }
  }
  W = diag($\vec \lambda$) \\
  Set $\Sigma = (X^TWX)^{-1}$ \\
  $\vec \beta_Z = \Sigma X^TWZ$ \\
  \BlankLine
  Sample $\beta^{(k)}|\vec Z,\vec \lambda,\vec Y \nu$ from $\mathcal{N}(\hat{\vec \beta_Z}^{(k)}, \textbf{$\Sigma$})$ \\
  Sample $\lambda_i$ from $\Gamma\left(\frac{\nu+1}{2},\frac{\nu+(Z_i-x_i^T\beta)^2}{2}\right)$
}
\end{algorithm} 

## Multinomial

The previous two data augmentation Gibbs samplers tackled regression models where the response is binary. Data augmentation Gibbs samplers can also be applied to multinomial response variables. Although there are approaches for both ordered and unordered categories, we will focus on the case of ordered categories. Once again, using a probit model will simplify the conditional posterior distributions. Before discussing the alogrithm in depth, we present the ordered multinomial probit [@aitchison1957;@gurland1960;@mccullagh1980] model for which we will use Gibbs Sampling to fit.


\begin{equation}
Y_i|\vec x_i, \beta \sim Multinomial(p_{ij})
\end{equation}
for $i = 1,..., n$ and $j = 1,..., J-1$.

\begin{equation}
p_{ij} = \Phi(\gamma_j - \vec x_i^T \vec \beta)
\end{equation}

\begin{equation}
\vec \gamma, \vec \beta \sim \pi(\vec \beta, \vec \gamma)
\end{equation}

where $\pi(\vec \beta, \vec \gamma)$ is the prior on the regression coeffeicents. We will assume to be non-informative as is commonly done in analysis. 

Similar to the binary case, a latent variable $\vec Z = (Z_1,...Z_n)$ is introduced making it possible to simulate from a joint posterior. Further, we can marginalize over $\vec Z$ to find the posterior distribution of $\beta|\vec{Z},\vec{Y},\vec \gamma$, which is the same multivarite normal distribution as the binary case, and the conditional posterior of $\vec \gamma|\vec{Z},\vec{Y}, \vec \beta$. This Gibbs sampler requires only sampling from truncated normal distributions, a multivarite distribution, and a unifrom distribution. [@albertchib93] The algorithm below presents, in detail, the sampling mechanism of this data augmentation Gibbs Sampler. 


### Algorithm:


\begin{algorithm}[H]
\caption{Ordered Multinomial Probit Regression Using Gibbs Sampler with Data Augmentation}
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\vec{Y}$,$\textbf{X}$}
\Output{Posterior Samples of Regression Coeffiecents, $\vec \beta$ and $\vec \gamma$}
\BlankLine
Set Number of Samples (Total ($N_s$) and Burn-in) \\
n = Number of Observations \\ 
Initailize $\vec \beta^{(0)}$ and $\vec \gamma^{(0)}$ as the MLE \\

\BlankLine
\For{k = 1 to $N_s$}{
  Sample $\vec \gamma_j$ from $Uniform(max(max(Z_i:Y_i = j),\gamma_{j-1}^{(k-1)},min(min(Z_i:Y_i = j+1),\gamma_{j+1}^{(k-1)}))$
  
  \For{i = 1 to n}{
        Sample $z_i^{(k)}|\vec \beta,\vec \gamma, y_i=j$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,1,$\gamma_{j-1},\gamma_j$)
    }
  Set $\Sigma = (X^TX)^{-1}$ \\
  Set $\vec \beta_Z = \Sigma X^TZ$ \\
  \BlankLine
    Sample $\beta^{(k)}|\vec Z,\vec \gamma,\vec Y$ from $\mathcal{N}(\hat{\vec \beta_Z}^{(k)}, \textbf{$\Sigma$})$
}
\end{algorithm}


## Implementation in Stan (NUTS)

# Results
<!-- (applying your algorithm to simulated or real data) -->

For three data sets, we apply the appropriate previously discussed model as well as standard maximum likelihood estimation and custom models written in Stan [@mcstan] and implemented in the RStan package [@rstan].

## Small-Cell Carcinoma

The small-cell carcinoma data comes from our STAT 211 class. Small-cell carcinoma of the lung is an aggressive cancer that can be treated with chemotherapy. Patients with small-cell carcinoma were randomly assigned to one of two therapy options. The outcome of the therapy is recorded on an ordinal scale (1: Progressive, 2: No Change, 3: Partial Remission, 4: Complete Remission). We fit multinomial ordered probit regression models with  with therapy option and sex as predictors of outcome. The model parameters are estimated with data augmented Gibbs sampling following the algorithm described in @albertchib93 and implemented in R, a custom model built in Stan, and traditional maximum likelihood estimation as implemented in the `polr` function in the MASS package [@MASS]. Both Bayesian methods are run to generate 4000 posterior samples, with the first 1000 discarded as burn-in samples, leaving 3000 samples for analysis. The Stan model took 1 minute and 13,9 seconds seconds to run, while the R model completed in 2.4 seconds.
<!-- 73.8619 -->

```{r Ordered Multinomial R}
ssc_gibbs_post <- read_rds(here("Small Cell Carcinoma", "ordered_multinomial.rds"))
post_sample_summary(ssc_gibbs_post) %>% 
  kable(caption = "Posterior sample summaries for small-cell carcinoma data using R-Gibbs")

trace_plot(ssc_gibbs_post) + ggtitle("Posterior Trace (Small-Cell Carcinoma Using R-Gibbs)")
dist_plot(ssc_gibbs_post) + ggtitle("Posterior Distribution (Small-Cell Carcinoma Using R-Gibbs)")
```

```{r Ordered Multinomial Stan}
multinomial_stan_post <- read_rds(here("Small Cell Carcinoma", "stan_samples_mp.rds")) %>% 
  as.matrix() %>% 
  as_tibble() %>% 
  select(-lp__) %>% 
  `colnames<-`(names(ssc_gibbs_post))
  
post_sample_summary(multinomial_stan_post) %>%
  kable(caption = "Posterior sample summaries for small-cell carcinoma data using Stan")

trace_plot(multinomial_stan_post) + ggtitle("Posterior Trace (Small-Cell Carcinoma Using Stan)")
dist_plot(multinomial_stan_post) + ggtitle("Posterior Distribution (Small-Cell Carcinoma Using Stan)")
```

```{r Ordered Multinomial MLE}
scc_dat <- read.table(here("Small Cell Carcinoma", "carcin.txt")) %>% 
  as_tibble() %>% 
  mutate(outcome = as_factor(outcome)) %>%
  uncount(count)

multinomial_mle_est <- MASS::polr(factor(outcome) ~ female + treatment, data = scc_dat, method = "probit")

suppressMessages(summary(multinomial_mle_est)) %>% 
  `$`(coefficients) %>% 
  as.data.frame() %>% 
  rownames_to_column("Variable") %>% 
  mutate(Variable = names(ssc_gibbs_post)) %>% 
  select(-"t value") %>% 
  rename(Estimate = Value) %>% 
  kable(caption = "MLE estimates for small-cell carcinoma data")
```



## Breast Cancer Data

We evaluate the probit regression methods with the Wisconsin Breast Cancer dataset, obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg [@wolberg1990multisurface]. The data reports 9 discrete measurements for 699 observations of clumps of breast cancer cells as well as the response variable, indicating whether or not the clump is malignant (1) or benign (0). The variables are Clump Thickness, Uniformity of Cell Size, Uniformity of Cell Shape, Marginal Adhesion, Single Epithelial Cell Size, Bare Nuclei, Bland Chromatin, Normal Nucleoli, and Mitoses. Only the Bare Nuclei variable included any missing data, so it was not included in this analysis. The model parameters are estimated with data-augmented Gibbs sampling following the algorithm described in @albertchib93 and implemented in R, a custom model built in Stan, and traditional maximum likelihood estimation as implemented in the `glm` function in R [@Rstats]. Both Bayesian methods are run to generate 4000 posterior samples, with the first 1000 discarded as burn-in samples, leaving 3000 samples for analysis. The Stan model took 6 minutes and 10.5 seconds seconds to run, while the R model completed in 7.8 seconds.

<!-- 310.555 -->

```{r Breast Cancer R}
bc_gibbs_post <- read_rds(here("Breast Cancer", "gibbs_samples_bc.rds"))
post_sample_summary(bc_gibbs_post) %>% 
  kable(caption = "Posterior sample summaries for breast cancer data using R-Gibbs")

trace_plot(bc_gibbs_post) + ggtitle("Posterior Trace (Breast Cancer Using R-Gibbs)")
dist_plot(bc_gibbs_post) + ggtitle("Posterior Distribution (Breast Cancer Using R-Gibbs)")
```


```{r Breast Cancer Stan}
bc_stan_post <- read_rds(here("Breast Cancer", "stan_samples_bc.rds")) %>% 
  as.matrix() %>% 
  as_tibble() %>% 
  select(-lp__) %>% 
  `colnames<-`(names(bc_gibbs_post))
  
post_sample_summary(bc_stan_post) %>%
  kable(caption = "Posterior sample summaries for breast cancer data using Stan")

trace_plot(bc_stan_post) + ggtitle("Posterior Trace (Breast Cancer Using Stan)")
dist_plot(bc_stan_post) + ggtitle("Posterior Distribution (Breast Cancer Using Stan)")
```


```{r Breast Cancer MLE, warning=FALSE}
dat_bc <- read_csv(here("Breast Cancer", "breast-cancer-wisconsin.data"),
                col_names = c('Sample code number',
                              'Clump Thickness',
                              'Uniformity of Cell Size',
                              'Uniformity of Cell Shape',
                              'Marginal Adhesion',
                              'Single Epithelial Cell Size',
                              'Bare Nuclei',
                              'Bland Chromatin',
                              'Normal Nucleoli',
                              'Mitoses',
                              'Class'), na = '?', ) %>% 
  select(-'Sample code number') %>% 
  select(-"Bare Nuclei") %>% 
  mutate(Class = Class / 2 - 1)

bc_mle_est <- glm(Class ~ ., family = binomial(link = "probit"), data = dat_bc)

summary(bc_mle_est)$coefficients %>% 
  as.data.frame() %>% 
  rownames_to_column("Variable") %>% 
  as_tibble() %>% 
  select("Variable", "Estimate", "Std. Error") %>% 
  kable(caption = "MLE estimates for breast cancer data")
```



## Baseball

We evaluate the performance of data-augmented Gibbs sampling on a larger datasets (more predictors and observations than the breast cancer data), by fitting probit regression models on baseball data. The baseball data comes from Major League Baseball pitch tracking software and was prepared by the Los Angeles Dodgers for the purpose of predicting whether or not a pitch was put into play. There are 100,000 pitches (observations) thrown by Dodger's starting pitchers and 19 features of each pitch, including velocity, spin rate, and location. The binary response indiactes whether or not a pitch was put into play (0: not put into play, 1: put into play). Missing data was removed, leaving 99,254 observations with 18 covariates for analysis. The model parameters are estimated with data augmented Gibbs sampling following the algorithm described in @albertchib93 and implemented in R and traditional maximum likelihood estimation as implemented in the `glm` function in R [@Rstats]. Both Bayesian methods are run to generate 5000 posterior samples, with the first 1000 discarded as burn-in samples, leaving 4000 samples for analysis. We also attempted to fit a custom model in Stan but the samples were generated very slowly (fewer than 500 samples per hour). In contrast, the R model generated all 5000 samples in 15 miutes and 41 seconds.

<!-- R: 941.138 -->
<!-- Stan: started at 5:28 -->

```{r Baseball R, warning=FALSE}
bb_gibbs_post <- read_rds(here("baseball", "baseball_samples.rds"))
post_sample_summary(bb_gibbs_post) %>% 
  kable(caption = "Posterior sample summaries for baseball data using R-Gibbs")

trace_plot(bb_gibbs_post) + ggtitle("Posterior Trace (Bawball Using R-Gibbs)")
dist_plot(bb_gibbs_post) + ggtitle("Posterior Distribution (Baseball Using R-Gibbs)")
```

```{r Baseball Stan, warning=FALSE}
# didn't happen
```

```{r Baseball MLE, warning=FALSE}
dat_bb <- read_csv(here("Baseball", "LAD_assessment_data.csv"),
                col_types = cols(batter_mlb_id = col_skip(), 
                                 pitcher_mlb_id = col_skip(), 
                                 venue_city = col_skip())) %>%
  drop_na()

bb_mle_est <- glm(is_in_play ~ ., family = binomial(link = "probit"), data = dat_bb)

summary(bb_mle_est)$coefficients %>% 
  as.data.frame() %>% 
  rownames_to_column("Variable") %>% 
  as_tibble() %>% 
  select("Variable", "Estimate", "Std. Error") %>% 
  kable(caption = "MLE estimates for baseball data")
```
<!-- There were 23 model parameters -->



# Discussion
<!-- (shortcomings of this new algorithm and how it can be improved). -->

Here's the discussion.

Mention Corey's research?

Extensions of data augmentation for other models?



Mention poor mixing?




\pagebreak

# References

::: {#refs}
:::

\pagebreak

# Appendix