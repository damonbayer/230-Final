---
title: "Gibbs Sampling with Data Augmentation for Bayesian Analysis of Binary and Polychotomous Response Data"
subtitle: "STAT 230 Final Report"
author: "Damon Bayer & Corey Katz"
date: "12/16/2019"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    df_print: kable
header-includes:
  - \usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, cache=T)
library(tidyverse)
library(here)
library(knitr)
source(here("plotting_functions.R"))

options(readr.num_columns = 0)

post_sample_summary <- function(post) {
  post %>% 
  pivot_longer(everything()) %>%
  mutate(name = as_factor(name)) %>% 
  rename(Variable = name) %>% 
  group_by(Variable) %>% 
  summarize_all(list(mean = mean,
                     sd = sd,
                     "2.5%" = function(x) quantile(x, probs = 0.025),
                     "50%" = median,
                     "97.5%" = function(x) quantile(x, probs = 0.975)))
}
```

# Abstract

Here's the abstract.

# Introduction
<!-- (explaining the motivation behind the new algorithm) -->

We cite @albertchib93

Here's the introduction.

# Methodology
<!-- (explaining the algorithm), -->

In this paper, we implement data augmentation Gibbs sampling for probit regression, multinomial probit regression of ordered data, and an extension of the probit regtession model, the t-link regression model [@albertchib93]. These methods are used to generate samples from posterior distributions for Bayesain inference. In this section, we will detail the methods and provide the algorithms for each. 

## Profit Model: 

The first model that data augmentation can be used to find posterior distributions is the profit model. The following is a standard Bayesian Probit model with non informative priors on the regression coefficients: 

$$y_i| \pi_{i},\vec{\beta}, \vec x_i \sim Bernoulli(\pi_{i})  \ \ \ \text{for} \ \  i = 0,...,n$$
$$ \Phi(\pi_{i}) = x_i^T\beta$$
$$\beta_j \sim N(0, 100),  \ \ \ \text{for} \  j = 0,...,p$$

Note that $\Phi$ is the cumulative density function of the normal distribution. 

The idea of data augmentation is to introduce a continuous latent variable derived from a binary response in order to make sampling from the posterior distributions of the profit model coefficients easier. By introducing independent latent variables $Z_1, ... Z_n$, we can now find the joint posterior of $\vec \beta$ and $\vec Z$ given Y. We can then marginalize over the posterior distribution of $\vec Z|\vec Y$ and thus we have the conditional posterior of $\beta|\vec{Z},\vec{Y}$. This method is further simplified if we assume (as we did above) non-informative priors on the regression coefficients, $\vec \beta$ [@albertchib93]. 

With this approach we are able to create a Gibbs sampler that only needs to sample from truncated normal distributions and multivariate normal distribution, which are extremely easy with today's computing power. Based on the fully conditonal posterior of $\beta|\vec{Z},\vec{Y}$, Albert and Chib equated this method of profit regression on binary Y to doing linear regression on the latent variable Z. This will be evident in the algortim as we use the `lm` function to find the mean of the posterior distribution of $\beta|\vec{Z},\vec{Y}$ [@albertchib93]. 

### Algorithm: 

\begin{algorithm}[H]
\caption{Probit Regression Using Gibbs Sampler with Data Augmentation}
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\vec{Y}$,$\textbf{X}$}
\Output{Posterior Samples of Regression Coeffiecents, $\vec \beta$}
\BlankLine
Set Number of Samples (Total ($N_s$) and Burn-in) 
Initailize $\vec \beta^{(0)}$ \\
Set $\Sigma = (X^TX)^{-1}$
n = Number of Observations
\BlankLine
\For{k = 1 to $N_s$}{
  \For{i = 1 to n}{
    \eIf{$y_i$ = 1}{
        Sample $z_i^{(k)}$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,1,0,$\infty$)\;
    }{
        Sample $z_i^{(k)}$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,1,$-\infty$,0)\;
    }
  }
  Regress $\vec Z$ onto $\textbf{X}$ to find $\vec \beta_Z$ \\
  Sample $\beta^{(k)}|\vec Z$ from $\mathcal{N}(\hat{\vec \beta_Z}^{(k)}, \textbf{$\Sigma$})$
  
  
}
\end{algorithm}
  
## T-link:

The T-link is an extension of the probit regression model. Instead of using the normal cdf, we use the t-distribution cdf as our link function. By generalizing the model, we can now choose a degrees of freedom, $\nu$ where t($\nu$) better fits out model and thus a link function that approximates other well-known link functions. For example, we are usually concerned with logistic regression because of the interpretability of the coefficients into odds ratios. The flexibility of the t-link function allows us to draw from a posterior distribution of $\vec \beta$ that is approximately the posterior distribution if we had chosen to use the logit link function. Note that if we set the degrees of freedom equal to infinity, we would revert back to the probit model [@albertchib93]. 

According to Albert and Chib, a t-distribution with a degrees of freedom of 8 is a fairly close approximation of the logistic regression model, once a correction factor of 0.634 is taken into account [@albertchib93].

To implement this link function, we must introduce a second set of latent variables, $\lambda_i$. This variable is introduced to simulate the heavier tales of the t-distribution compared to the normal distribution. Our implementation is slightly different from Albert and Chib's because of the ability to sample from a truncated t distribution and issues implementing their algorithm. We still need to simulate $\lambda$ in order to sample from the conditional posterior of $\beta^{(k)}|\vec Z, \vec Y, \lambda, \nu$. We took the degrees of freedom equal to 8 to campare results to the logistic regression model. You could sample from the posterior distribution of $\nu$ and better understand how well your model fits the data, but we choose to focus on logistic regression. 


### Algorithm:

\begin{algorithm}[H]
\caption{Tobit Regression Using Gibbs Sampler with Data Augmentation ($\nu =8$)}
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\vec{Y}$,$\textbf{X}$}
\Output{Posterior Samples of Regression Coeffiecents, $\vec \beta$}
\BlankLine
Set Number of Samples (Total ($N_s$) and Burn-in) \\
n = Number of Observations \\ 
Initailize $\vec \beta^{(0)}$ and set $\lambda = \vec 1_n$ \\
$\nu=8$
\BlankLine
\For{k = 1 to $N_s$}{
  \For{i = 1 to n}{
    \eIf{$y_i$ = 1}{
        Sample $z_i^{(k)}$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,$\lambda_i^{-1}$,0,$\infty$)\;
    }{
        Sample $z_i^{(k)}$ from trunc$\mathcal{N}$($x_i^T\beta^{(k-1)}$,$\lambda_i^{-1}$,$-\infty$,0)\;
    }
  }
  W = diag($\vec \lambda$) \\
  Set $\Sigma = (X^TWX)^{-1}$ \\
  $\vec \beta_Z = \Sigma X^TWZ$ \\
  \BlankLine
  Sample $\beta^{(k)}|\vec Z,\vec \lambda,\vec Y \nu$ from $\mathcal{N}(\hat{\vec \beta_Z}^{(k)}, \textbf{$\Sigma$})$ \\
  Sample $\lambda_i$ from $\Gamma\left(\frac{\nu+1}{2},\frac{\nu+(Z_i-x_i^T\beta)^2}{2}\right)$
}
\end{algorithm} 

Multinomial:

Description

Algorithm.


# Results
<!-- (applying your algorithm to simulated or real data) -->

For three data sets, we apply the appropriate previously discussed model as well as standard maximum likelihood estimation and custom models written in Stan [@mcstan] and implemented in the RStan package [@rstan].

## Small-Cell Carcinoma

The small-cell carcinoma data comes from our STAT 211 class. Small-cell carcinoma of the lung is an aggressive cancer that can be treated with chemotherapy. Patients with small-cell carcinoma were randomly assigned to one of two therapy options. The outcome of the therapy is recorded on an ordinal scale (1: Progressive, 2: No Change, 3: Partial Remission, 4: Complete Remission). We fit multinomial ordered probit regression models with  with therapy option and sex as predictors of outcome. The model parameters are estimated with data augmented Gibbs sampling following the algorithm described in @albertchib93 and implemented in R, a custom model built in Stan, and traditional maximum likelihood estimation as implemented in the `polr` function in the MASS package [@MASS]. Both Bayesian methods are run to generate 4000 posterior samples, with the first 1000 discarded as burn-in samples, leaving 3000 samples for analysis. The Stan model took 1 minute and 13,9 seconds seconds to run, while the R model completed in 2.4 seconds.
<!-- 73.8619 -->

```{r Ordered Multinomial R}
ssc_gibbs_post <- read_rds(here("ordered_multinomial.rds"))
post_sample_summary(ssc_gibbs_post) %>% 
  kable(caption = "Posterior sample summaries for small-cell carcinoma data using R-Gibbs")

trace_plot(ssc_gibbs_post) + ggtitle("Posterior Trace (Small-Cell Carcinoma Using R-Gibbs)")
dist_plot(ssc_gibbs_post) + ggtitle("Posterior Distribution (Small-Cell Carcinoma Using R-Gibbs)")
```

```{r Ordered Multinomial Stan}
multinomial_stan_post <- read_rds(here("stan_samples_mp.rds")) %>% 
  as.matrix() %>% 
  as_tibble() %>% 
  select(-lp__) %>% 
  `colnames<-`(names(ssc_gibbs_post))
  
post_sample_summary(multinomial_stan_post) %>%
  kable(caption = "Posterior sample summaries for small-cell carcinoma data using Stan")

trace_plot(multinomial_stan_post) + ggtitle("Posterior Trace (Small-Cell Carcinoma Using Stan)")
dist_plot(multinomial_stan_post) + ggtitle("Posterior Distribution (Small-Cell Carcinoma Using Stan)")
```

```{r Ordered Multinomial MLE}
scc_dat <- read.table(here("Data", "carcin.txt")) %>% 
  as_tibble() %>% 
  mutate(outcome = as_factor(outcome)) %>%
  uncount(count)

multinomial_mle_est <- MASS::polr(factor(outcome) ~ female + treatment, data = scc_dat, method = "probit")

suppressMessages(summary(multinomial_mle_est)) %>% 
  `$`(coefficients) %>% 
  as.data.frame() %>% 
  rownames_to_column("Variable") %>% 
  mutate(Variable = names(ssc_gibbs_post)) %>% 
  select(-"t value") %>% 
  rename(Estimate = Value) %>% 
  kable(caption = "MLE estimates for small-cell carcinoma data")
```



## Breast Cancer Data

We evaluate the probit regression methods with the Wisconsin Breast Cancer dataset, obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg [@wolberg1990multisurface]. The data reports 9 discrete measurements for 699 observations of clumps of breast cancer cells as well as the response variable, indicating whether or not the clump is malignant (1) or benign (0). The variables are Clump Thickness, Uniformity of Cell Size, Uniformity of Cell Shape, Marginal Adhesion, Single Epithelial Cell Size, Bare Nuclei, Bland Chromatin, Normal Nucleoli, and Mitoses. Only the Bare Nuclei variable included any missing data, so it was not included in this analysis. The model parameters are estimated with data-augmented Gibbs sampling following the algorithm described in @albertchib93 and implemented in R, a custom model built in Stan, and traditional maximum likelihood estimation as implemented in the `glm` function in R [@Rstats]. Both Bayesian methods are run to generate 4000 posterior samples, with the first 1000 discarded as burn-in samples, leaving 3000 samples for analysis. The Stan model took 6 minutes and 10.5 seconds seconds to run, while the R model completed in 7.8 seconds.

<!-- 310.555 -->



```{r Breast Cancer R}
bc_gibbs_post <- read_rds(here("gibbs_samples_bc.rds"))
post_sample_summary(bc_gibbs_post) %>% 
  kable(caption = "Posterior sample summaries for breast cancer data using R-Gibbs")

trace_plot(bc_gibbs_post) + ggtitle("Posterior Trace (Breast Cancer Using R-Gibbs)")
dist_plot(bc_gibbs_post) + ggtitle("Posterior Distribution (Breast Cancer Using R-Gibbs)")
```


```{r Breast Cancer Stan}
bc_stan_post <- read_rds(here("stan_samples_bc.rds")) %>% 
  as.matrix() %>% 
  as_tibble() %>% 
  select(-lp__) %>% 
  `colnames<-`(names(bc_gibbs_post))
  
post_sample_summary(bc_stan_post) %>%
  kable(caption = "Posterior sample summaries for breast cancer data using Stan")

trace_plot(bc_stan_post) + ggtitle("Posterior Trace (Breast Cancer Using Stan)")
dist_plot(bc_stan_post) + ggtitle("Posterior Distribution (Breast Cancer Using Stan)")
```


```{r Breast Cancer MLE, warning=FALSE}
dat_bc <- read_csv(here("Data", "breast-cancer-wisconsin.data"),
                col_names = c('Sample code number',
                              'Clump Thickness',
                              'Uniformity of Cell Size',
                              'Uniformity of Cell Shape',
                              'Marginal Adhesion',
                              'Single Epithelial Cell Size',
                              'Bare Nuclei',
                              'Bland Chromatin',
                              'Normal Nucleoli',
                              'Mitoses',
                              'Class'), na = '?', ) %>% 
  select(-'Sample code number') %>% 
  select(-"Bare Nuclei") %>% 
  mutate(Class = Class / 2 - 1)

bc_mle_est <- glm(Class ~ ., family = binomial(link = "probit"), data = dat_bc)

summary(bc_mle_est)$coefficients %>% 
  as.data.frame() %>% 
  rownames_to_column("Variable") %>% 
  as_tibble() %>% 
  select("Variable", "Estimate", "Std. Error") %>% 
  kable(caption = "MLE estimates for breast cancer data")
```



## Baseball

We evaluate the performance of data-augmented Gibbs sampling on a larger datasets (more predictors and observations than the breast cancer data), by fitting probit regression models on baseball data. The baseball data comes from Major League Baseball pitch tracking software and was prepared by the Los Angeles Dodgers for the purpose of predicting whether or not a pitch was put into play. There are 100,000 pitches (observations) thrown by Dodger's starting pitchers and 19 features of each pitch, including velocity, spin rate, and location. The binary response indiactes whether or not a pitch was put into play (0: not put into play, 1: put into play). Missing data was removed, leaving 99,254 observations with 18 covariates for analysis. The model parameters are estimated with data augmented Gibbs sampling following the algorithm described in @albertchib93 and implemented in R and traditional maximum likelihood estimation as implemented in the `glm` function in R [@Rstats]. Both Bayesian methods are run to generate 5000 posterior samples, with the first 1000 discarded as burn-in samples, leaving 4000 samples for analysis. We also attempted to fit a custom model in Stan but the samples were generated very slowly (fewer than 500 samples per hour). In contrast, the R model generated all 5000 samples in 15 miutes and 41 seconds.

<!-- R: 941.138 -->
<!-- Stan: started at 5:28 -->

```{r Baseball R, warning=FALSE}
bb_gibbs_post <- read_rds(here("baseball_samples.rds"))
post_sample_summary(bb_gibbs_post) %>% 
  kable(caption = "Posterior sample summaries for baseball data using R-Gibbs")

trace_plot(bb_gibbs_post) + ggtitle("Posterior Trace (Bawball Using R-Gibbs)")
dist_plot(bb_gibbs_post) + ggtitle("Posterior Distribution (Baseball Using R-Gibbs)")
```

```{r Baseball Stan, warning=FALSE}
# didn't happen
```

```{r Baseball MLE, warning=FALSE}
dat_bb <- read_csv(here("Data", "LAD_assessment_data.csv"),
                col_types = cols(batter_mlb_id = col_skip(), 
                                 pitcher_mlb_id = col_skip(), 
                                 venue_city = col_skip())) %>%
  drop_na()

bb_mle_est <- glm(is_in_play ~ ., family = binomial(link = "probit"), data = dat_bb)

summary(bb_mle_est)$coefficients %>% 
  as.data.frame() %>% 
  rownames_to_column("Variable") %>% 
  as_tibble() %>% 
  select("Variable", "Estimate", "Std. Error") %>% 
  kable(caption = "MLE estimates for baseball data")
```
<!-- There were 23 model parameters -->



# Discussion
<!-- (shortcomings of this new algorithm and how it can be improved). -->

Here's the discussion.

\pagebreak

# References

::: {#refs}
:::

\pagebreak

# Appendix